{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading 93mwordnet: Package '93mwordnet' not found\n",
      "[nltk_data]     in index\n",
      "[nltk_data] Downloading package stopwords to /Users/aitor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/aitor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import nltk\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "nltk.download('93mwordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Fecha         Usuario      Displayed name  \\\n",
      "0 2015-11-14 23:59:59  alysoncummings     Alyson Cummings   \n",
      "1 2015-11-14 23:59:56      feedingjoy  Joy is a Lifestyle   \n",
      "2 2015-11-14 23:59:55  StreamReporter     Stream Reporter   \n",
      "3 2015-11-14 23:59:53  StreamReporter     Stream Reporter   \n",
      "4 2015-11-14 23:59:48        Gagateux   AVEC LILY DEAR ðŸ‡«ðŸ‡·   \n",
      "\n",
      "                                           Contenido label  \n",
      "0  pianist play john lennon imagine outside batac...   NaN  \n",
      "1  happiness someone cycled bataclan paris play i...   NaN  \n",
      "2  periscope rosscaneva rt mitney rosscaneva peri...   NaN  \n",
      "3  periscope rosscaneva rt jmoon rosscaneva peris...   NaN  \n",
      "4  mathias amp marie died bataclan yesterday didn...   NaN  \n"
     ]
    }
   ],
   "source": [
    "# cargar excel con tweets\n",
    "ruta = '/Users/aitor/Library/CloudStorage/OneDrive-UNIR/ASIGNATURAS/TFG/datasets/checkMix.xlsx'\n",
    "df = pd.read_excel(ruta)\n",
    "\n",
    "# convertir a lista\n",
    "tweets = df.values.tolist()\n",
    "\n",
    "df = pd.DataFrame(tweets, columns=['Fecha', 'Usuario', 'Displayed name', 'Contenido', 'label'])\n",
    "\n",
    "print(df.head())\n",
    "# Limpiar los tweets\n",
    "\n",
    "# Eliminar filas con la columna 'Contenido' vacÃ­a\n",
    "df.dropna(subset=['Contenido'], inplace=True)\n",
    "\n",
    "# Eliminar filas duplicadas en todo el dataset\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Restablecer el Ã­ndice despuÃ©s de eliminar filas\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df['Contenido'] = df['Contenido'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))\n",
    "df['Contenido'] = df['Contenido'].apply(lambda x: re.sub(r'@(\\w+)', '', str(x)))\n",
    "df['Contenido'] = df['Contenido'].apply(lambda x: re.sub(r'#', '', str(x)))\n",
    "df['Contenido'] = df['Contenido'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n",
    "df['Contenido'] = df['Contenido'].apply(lambda x: re.sub(r'\\d+', '', str(x)))\n",
    "df['Contenido'] = df['Contenido'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', str(x)))\n",
    "df['Contenido'] = df['Contenido'].apply(lambda x: re.sub(r'\\s+', ' ', str(x)))\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# remove stopwords and lemmatize tokens using apply() method\n",
    "df['Contenido'] = df['Contenido'].apply(lambda x: x.lower())\n",
    "df['Contenido'] = df['Contenido'].apply(lambda x: word_tokenize(x))\n",
    "df['Contenido'] = df['Contenido'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "df['Contenido'] = df['Contenido'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "df['Contenido'] = df['Contenido'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# remove timezone information using tz_localize() method\n",
    "df['Fecha'] = pd.to_datetime(df['Fecha']).dt.tz_localize(None)\n",
    "## Guardar los tweets limpios en un archivo Excel ##\n",
    "# ruta guardado\n",
    "# y guardar los tweets en un archivo Excel\n",
    "filename = \"checkMixClean.xlsx\"\n",
    "ruta_carpeta = 'resultados/terrorismo/'\n",
    "ruta_completa = ruta_carpeta + filename\n",
    "# transfiero el dataframe a un archivo excel\n",
    "df.to_excel(ruta_completa, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
